{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec46805c",
   "metadata": {},
   "source": [
    "## 3. Natural Tweet Acquisition\n",
    "To obtain a selection of natural tweets that we can use for a control group, we will use the Tweepy library, which provides access to the Twitter API. It includes the ability to search for specific keywords, get tweets from specific users, and a number of other useful functions.\n",
    "\n",
    "Our path here will be as follows:\n",
    "1. Iterate through the named entities we identified in the previous notebook, using the search API to get a decent number of tweets on each entity.\n",
    "2. For each user identified in step #1, pull a number of past tweets from that user to create a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855bcc5",
   "metadata": {},
   "source": [
    "### 3.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f50d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import utilities.tweet_utils as tweet_utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e7265",
   "metadata": {},
   "source": [
    "Access to the Twitter API through Tweepy requires a number of keys and tokens. In order to obtain access, you will need to go to the [Twitter Developer Portal](https://developer.twitter.com/en/portal/dashboard) and sign up. Twitter requires an application for access, but this can be obtained if you have a legitimate project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'j2nAmqKcjV0Vw4vftNrEHjYN0'\n",
    "API_KEY_SECRET = 'HRCx48ogjz88nv1IFwOhZYuPjkNh1bMx3WY2GU7rmwr7bV9IDo'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAANQOaAEAAAAAVUQvu%2Fkc6A%2Bh3Mp2ZnDX4%2FQmceM%3DZ3TEtjYZhKNNt1FwS5pPc0t2bBYlvhx0jHrjR6itGAiMD9hOMR'\n",
    "ACCESS_TOKEN = '3409085501-gWVxnfOYeZGaqb9ub272PXNC8nvizlWDTpk6BEW'\n",
    "ACCESS_TOKEN_SECRET = 'ln8RicUizrYgCMkJRXRGEYLhoDEXY6KJmSZGuD9BseQJp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12404284",
   "metadata": {},
   "source": [
    "Here we set up API access and authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_KEY_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda926f",
   "metadata": {},
   "source": [
    "### 3.2 Load Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c7ace",
   "metadata": {},
   "source": [
    "We use this function to clean up the entities identified from the NER model. We will then use those to search Twitter for related tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clean_entities(entity_df,min_count=10):\n",
    "    \"\"\"Cleans up entities from NER step\n",
    "\n",
    "    Args:\n",
    "        entity_df (dataframe): Contains entities, entity type, and count\n",
    "        min_count (int): Minimum count of mentions of each entity to include\n",
    "\n",
    "    Returns:\n",
    "        entity_df (dataframe): Cleaned dataframe\n",
    "    \"\"\"\n",
    "    entity_df = entity_df.sort_values(by='count',ascending=False)\n",
    "    entity_df['len'] = entity_df['word'].map(lambda x: len(str(x)))\n",
    "    entity_df = entity_df.drop(entity_df[entity_df['count']<min_count].index)\n",
    "    entity_df = entity_df.drop(entity_df[entity_df['len']<2].index)\n",
    "    entity_df = entity_df.drop(entity_df[entity_df['word']=='RT'].index)\n",
    "    entity_df = entity_df.drop(entity_df[entity_df['word'].str.contains(r'[@#&$%+-/*]')].index)\n",
    "\n",
    "    return entity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bf707",
   "metadata": {},
   "source": [
    "#### 3.2.1 Chinese Tweet Entities\n",
    "The following lines simply gather and display the entities to be included in the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015809ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_entities = pd.read_csv('../working_files/chinese_entities.csv')\n",
    "chinese_entities = filter_clean_entities(chinese_entities,min_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chinese_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe57a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9318b4",
   "metadata": {},
   "source": [
    "#### 3.2.2 Russian Tweet Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6629c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_entities = pd.read_csv('../working_files/russian_entities.csv')\n",
    "russian_entities = filter_clean_entities(russian_entities,min_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(russian_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4809947",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_entities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c79921",
   "metadata": {},
   "source": [
    "#### 3.2.3 Combined Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2937839",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_entities = pd.concat([chinese_entities, russian_entities],axis=0)\n",
    "combined_entities = combined_entities.drop_duplicates(subset=['word'])\n",
    "combined_entities.to_csv('../working_files/state_operator_entities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b43986",
   "metadata": {},
   "source": [
    "#### 3.2.4 Final Processing\n",
    "After creating this list, I did some manual marking of several duplicate and/or misspelled entries in Excel. The manually edited list is named `state_operator_entities_edited.csv`. Entries to be excluded are marked with a `1` in the `exclude` column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14eb6d",
   "metadata": {},
   "source": [
    "### 3.3 Gather Search Sample\n",
    "We will now run through the entities collected above and use each one as a search query via the Twitter API. Below we collect 100 samples for each query, and save them in the same file. Once we identify the users making these tweets, we will then examine their timelines for recent tweets so that we can build our tweet sequences for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_searcher(query, client, filename, max_results=100):\n",
    "    \"\"\"Searches and writes tweets from the last 7 days using given search term\n",
    "    \n",
    "    Args:\n",
    "        topic (string): Query to be passed to the Twitter API\n",
    "        client (tweepy.Client): Client object\n",
    "        filename (string): Name of file to write tweets to\n",
    "    \n",
    "    \"\"\"\n",
    "    response = client.search_recent_tweets(query, max_results=max_results,\n",
    "                tweet_fields=['id','author_id','created_at','lang','text'])\n",
    "    # The search_recent_tweets method returns a Response object, a named tuple \n",
    "    # with data, includes, errors, and meta fields\n",
    "    print(query)\n",
    "\n",
    "    # In this case, the data field of the Response returned is a list of Tweet\n",
    "    # objects\n",
    "    tweets = response.data\n",
    "\n",
    "    # Each Tweet object has default ID and text fields\n",
    "    for tweet in tweets:\n",
    "        with open(filename, 'a') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(\n",
    "                [\n",
    "                    tweet.id,\n",
    "                    tweet.author_id,\n",
    "                    tweet.created_at,\n",
    "                    tweet.lang,\n",
    "                    tweet.text\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = pd.read_csv('../working_files/state_operator_entities_edited.csv',index_col=0)\n",
    "ent = ent[ent['exclude']==0]\n",
    "len(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939fb4e",
   "metadata": {},
   "source": [
    "This code will loop through the topics and conduct a search for each one, excluding retweets in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(BEARER_TOKEN)\n",
    "SAMPLE_DESTINATION = '../working_files/tweets_05.16.2022.csv'\n",
    "\n",
    "for t in ent['word']:\n",
    "    q = t + ' -is:retweet lang:en' # \"-is:retweet\" excludes retweets\n",
    "    tweet_searcher(q, client, SAMPLE_DESTINATION,max_results=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce87f6",
   "metadata": {},
   "source": [
    "### 3.4 Pull Recent Tweets From Users\n",
    "The following cells will pull the last N tweets from the unique users identified in the query results above. This will be used to build the Tweet sequences we will use for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e849a",
   "metadata": {},
   "source": [
    "#### 3.4.1 Load Query Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd12cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query results\n",
    "df = pd.read_csv(SAMPLE_DESTINATION,\n",
    "                 names=['id','author_id','created_at','lang','text'], \n",
    "                 index_col=False)\n",
    "df = df[df['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d10084",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.unique(df['author_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284d682",
   "metadata": {},
   "source": [
    "#### 3.4.2 Pull Screennames\n",
    "Unfortunately, there is no option to return screen names from the above API calls, and there is no way to pull recent tweets (as we will need to below) from users using `author_id`. Because of this, we will need to manually get the screen names from the author IDs found above.\n",
    "\n",
    "Here we will carry out that process. This step will take over 2.5 hours using the default settings, due to Twitter's API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26459413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989b6cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def collect_twitter_user_screennames(author_ids):\n",
    "    \"\"\"Pulls screennames from Twitter API based on author_id\n",
    "\n",
    "    Args:\n",
    "        author_ids (list of ints): author_ids as provided by Twitter\n",
    "\n",
    "    Returns:\n",
    "        screennames (list of strings): screennames corresponding to each author \n",
    "\n",
    "    \"\"\"\n",
    "    screen_names = []\n",
    "    for s in range(0,len(author_ids),900):\n",
    "        # API is rate-limited to 900 user lookups every 15 minutes \n",
    "        end = len(author_ids) if s+900>=len(author_ids) else s+900\n",
    "        print(f\"Acquiring range {s} to {end}\")\n",
    "        for id in users[s:s+900]:\n",
    "            try:\n",
    "                screen_names.append(api.get_user(id=id).screen_name)\n",
    "            except:\n",
    "                print(\"Error collecting user name. User omitted.\")\n",
    "        time.sleep(60*15)\n",
    "    return screen_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f074fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "screennames = collect_twitter_user_screennames(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce743ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(screennames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c417fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save list to use later, if needed\n",
    "with open('../working_files/screen_names.pkl', 'wb') as handle:\n",
    "    pickle.dump(screennames, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495fd56",
   "metadata": {},
   "source": [
    "#### 3.4.3 Pull Recent Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(username, filename, tweet_limit=50):\n",
    "    \"\"\"Pulls the last N tweets from a user's timeline and writes them to a file\n",
    "    \n",
    "    Args:\n",
    "        username (string): Screen name of user whose timeline is to be searched\n",
    "        filename (string): Name of file to which tweets are to be appended\n",
    "        tweet_limit (int): Maximum number of tweets to retrieve from user\n",
    "    \n",
    "    Returns:\n",
    "        Nothing\n",
    "    \n",
    "    \"\"\"\n",
    "    csv_file = open(filename, \"a\")\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Authorization to consumer key and consumer secret\n",
    "    auth = tweepy.OAuthHandler(API_KEY, API_KEY_SECRET)\n",
    "\n",
    "    # Access to user's access key and access secret\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "    # Calling api\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    print(username)\n",
    "    # Get tweets\n",
    "    for tweet in tweepy.Cursor(\n",
    "        api.user_timeline, \n",
    "        screen_name=username,\n",
    "        include_rts=False\n",
    "    ).items(limit=tweet_limit):\n",
    "        csv_writer.writerow(\n",
    "            [\n",
    "                tweet.id,\n",
    "                tweet.author.screen_name,\n",
    "                tweet.created_at,\n",
    "                tweet.lang,\n",
    "                tweet.source,\n",
    "                tweet.retweet_count,\n",
    "                tweet.favorited,\n",
    "                tweet.retweeted,\n",
    "                tweet.text\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64a2c8",
   "metadata": {},
   "source": [
    "This loop will run for a while and then sleep once it reaches the Twitter API rate limit. After sleeping for a certain amount of time, it will resume. This process will repeat until all the Tweets are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1278bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMELINE_TWEETS_DESTINATION = \"../working_files/timeline_tweets_05.16.2022.csv\"\n",
    "\n",
    "for i in range(len(screennames)):\n",
    "    try:\n",
    "        get_tweets(screennames[i],TIMELINE_TWEETS_DESTINATION,50)\n",
    "    except:\n",
    "        \"Error detected. Moving to next user.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb87e32",
   "metadata": {},
   "source": [
    "### 3.5 Clean & Combine Tweets\n",
    "Now that we have obtained a large sample of user tweets on the same topics discussed by our state operators, we will apply the same cleaning and filtering process to them as we did to the state operator tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff044fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.tweet_utils as tweet_utils\n",
    "\n",
    "rts = pd.read_csv(TIMELINE_TWEETS_DESTINATION,\n",
    "                  names=['id','userid','tweet_time','tweet_language','source',\n",
    "                         'retweet_count','favorited','retweeted','tweet_text'], \n",
    "                  index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1661dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts = rts[rts['tweet_language']=='en']\n",
    "rts.shape, len(pd.unique(rts['userid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5342f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts = tweet_utils.apply_filters(rts)\n",
    "rts.shape, len(pd.unique(rts['userid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts = tweet_utils.combine_tweets(rts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts.iloc[11444,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['userid','tweet_text','tweet_time','clean_tweets','recent_tweets']\n",
    "\n",
    "rts = rts[final_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts.sample(100).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579931cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts.to_csv('../working_files/real_tweet_sequences.csv',sep=',', quotechar='\"',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbdf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "715799c5962b37cda2a43057b7b6f81562e2f9a9ac0bada816b8b08e4df60984"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch-dl-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
