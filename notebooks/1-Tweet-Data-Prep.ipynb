{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import utilities.tweet_utils as tweet_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16cd50",
   "metadata": {},
   "source": [
    "## 1. Load State Information Operator Tweets from Twitter Data Dump\n",
    "In this notebook, we will take a selection of tweets taken from known state operators and released by Twitter Information Operations. The published archives are available [here](https://transparency.twitter.com/en/reports/information-operations.html). Our objective is to clean and process these tweets, removing retweets, so that we can train a model to detect tweet sequences that identify state operators.\n",
    "\n",
    "The intended use case is that, given any tweet, we will be able to take the N (9 in this case) preceding tweets from that user and identify them as state operators or normal users.\n",
    "\n",
    "Data dumps should be downloaded to the `raw_downloads/[language]` folder and extracted there. The `merge_csvs_on_columns` function will load all the CSV files in this folder (assuming each file has the same column names) and concatenate them into a dataframe. Empty tweets will be dropped.\n",
    "\n",
    "In this case, I have downloaded and extracted all of the Russian and Chinese data dumps back to 2019. Where the ZIP folders included files going back several years, I only included 2019-2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494bc36",
   "metadata": {},
   "source": [
    "### 1.1 Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5acf389",
   "metadata": {},
   "source": [
    "Our first order of business is to create a number of helper functions that we can use to load, combine, clean, and assemble data for more thorough analysis and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcca243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csvs_on_columns(data_dir, columns):\n",
    "    \"\"\"Merges downloaded Tweet CSVs in target folder, dropping NAs\n",
    "\n",
    "    Args:\n",
    "        data_dir (string): Name of directory in which CSVs are located\n",
    "        columns (list of strings): List of columns in target CSVs\n",
    "\n",
    "    Returns:\n",
    "        Pandas dataframe: The merged CSVs in the target directory    \n",
    "    \"\"\"\n",
    "    filenames = [name for name in os.listdir(data_dir) \n",
    "                 if os.path.splitext(name)[-1]=='.csv']\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for fname in filenames:\n",
    "        # remove any rows with blank Tweets\n",
    "        tmp_df = pd.read_csv(os.path.join(data_dir,fname)).dropna(\n",
    "            subset=['tweet_text'])\n",
    "        tmp_df = tmp_df[columns]\n",
    "        df = df.append(tmp_df, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06fd244",
   "metadata": {},
   "source": [
    "### 1.2 Process Russian Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42529307",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../raw_downloads/russian/'\n",
    "cols = ['userid','user_profile_description','tweet_text','tweet_time',\n",
    "        'tweet_language','is_retweet','hashtags','urls']\n",
    "df = merge_csvs_on_columns(dir, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b91969",
   "metadata": {},
   "source": [
    "#### 1.2.1 Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-English Tweets\n",
    "df = df[df['tweet_language']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae278fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27752f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, len(pd.unique(df['userid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8480b4c",
   "metadata": {},
   "source": [
    "This yields us quite a few tweets, all from 2019-2021. It should certainly be enough to at least test our capability to train a model.\n",
    "\n",
    "As a next step, we will remove all retweets, since the text of retweets is likely to come from natural sources (news and so forth). We want to exclude these from the training so that the language model is trained only on the language patterns that come from text the state operators themselves wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit1 = df[\"is_retweet\"] == False\n",
    "crit2 = ~df[\"tweet_text\"].str.startswith(\"RT\")\n",
    "\n",
    "df = df[crit1 & crit2].copy()\n",
    "df.shape, len(pd.unique(df['userid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d36fe",
   "metadata": {},
   "source": [
    "Next, we'll clean the text using a custom function implemented in the `tweet_utils` folder. This applies a series of regex operations to clean up special characters, @ mentions, newlines, URLs, and so forth. We'll also add a word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f357d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_tweets\"] = (df[\"tweet_text\"].map(lambda text: tweet_utils.clean_text(text)))\n",
    "df['word_count'] = df['clean_tweets'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e237bbb",
   "metadata": {},
   "source": [
    "Finally, we'll get rid of empty or almost-empty tweets with the following filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb16ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit1 = ~df[\"clean_tweets\"].isnull()\n",
    "crit2 = df[\"clean_tweets\"] != \"\"\n",
    "crit3 = df[\"word_count\"] > 3\n",
    "\n",
    "df = df[crit1 & crit2 & crit3].copy()\n",
    "df.shape, len(pd.unique(df['userid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec325c8",
   "metadata": {},
   "source": [
    "#### 1.2.2 Create Tweet Sequences\n",
    "If we want to determine whether a user is a state operator, it is unlikely that one single tweet will provide us with enough data. State operators often post on a variety of topics (including sharing memes and links likely to be popular) in order to disguise their activity and gain followers. In order to effectively assess a user, we'll train our model on multiple sequences of N tweets. For this iteration, we've chosen `n=10`. The custom function below does the following:\n",
    " 1. Sort tweets by user and then date.\n",
    " 2. For each tweet, find the 9 previous tweets and concatenate them in backwards-chronological order.\n",
    " 3. Add sequence of 10 tweets to the dataframe on the row of the most recent tweet in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweet_utils.combine_tweets(df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2082809",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[11444,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['userid','tweet_text','tweet_time','clean_tweets','recent_tweets']\n",
    "\n",
    "df = df[final_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61643fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../working_files/russian_tweet_sequences.csv',sep=',', quotechar='\"',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f5a32",
   "metadata": {},
   "source": [
    "### 1.3 Process Chinese Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b34740",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../raw_downloads/chinese/'\n",
    "cols = ['userid','user_profile_description','tweet_text','tweet_time','tweet_language',\n",
    "        'is_retweet','hashtags','urls']\n",
    "df = merge_csvs_on_columns(dir, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['tweet_language']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, len(pd.unique(df['userid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a93da2",
   "metadata": {},
   "source": [
    "The criteria used to filter the Russian tweets above have been added to the `tweet_utils` file, so we will use that here for brevity. The `apply_filters` function will also call the `clean_text` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a26631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweet_utils.apply_filters(df)\n",
    "df.shape, len(pd.unique(df['userid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de7bb5",
   "metadata": {},
   "source": [
    "We also apply the same combination logic to the Chinese operator tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweet_utils.combine_tweets(df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2685662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[11444,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = ['userid','tweet_text','tweet_time','clean_tweets','recent_tweets']\n",
    "\n",
    "df = df[final_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf71f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15582d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../working_files/chinese_tweet_sequences.csv',sep=',', quotechar='\"',header=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "715799c5962b37cda2a43057b7b6f81562e2f9a9ac0bada816b8b08e4df60984"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch-dl-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
